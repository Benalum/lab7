{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install torch torchtext transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uy1Z5AiuiY6-",
        "outputId": "8fd5da28-7d48-4977-a67a-2335f7861c01"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting torchtext\n",
            "  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.26.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.8.30)\n",
            "Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torchtext\n",
            "Successfully installed torchtext-0.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load IMDb dataset\n",
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xzf aclImdb_v1.tar.gz\n",
        "\n"
      ],
      "metadata": {
        "id": "ov_ch2usmTCt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2806c9b-2e58-4962-963b-0ab54731b014"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-11-19 07:02:51--  https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
            "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
            "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 84125825 (80M) [application/x-gzip]\n",
            "Saving to: ‘aclImdb_v1.tar.gz.1’\n",
            "\n",
            "aclImdb_v1.tar.gz.1 100%[===================>]  80.23M  3.68MB/s    in 26s     \n",
            "\n",
            "2024-11-19 07:03:18 (3.07 MB/s) - ‘aclImdb_v1.tar.gz.1’ saved [84125825/84125825]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Fine tuning a pre-trained BERT\n",
        "\n",
        "This code is fine-tuning a pre-trained BERT model on the IMDb dataset for sentiment analysis. It starts with a BERT model that is already pre-trained on a large text corpus (from bert-base-uncased), which has learned general language representations. The code loads this pre-trained model and then trains it further on the IMDb dataset specifically for sentiment analysis (a binary classification task). This process adjusts the weights of the model to better predict positive or negative sentiment based on the IMDb data.\n",
        "\n",
        "Training BERT from scratch would mean initializing a BERT model with random weights and training it on a very large corpus (like Wikipedia or BooksCorpus) to learn language representations, which is computationally intensive. Fine-tuning, on the other hand, starts from a pre-trained model and requires much less data and computation. So, this code fine-tunes the model rather than training from scratch. All layers of BERT are trainable during fine-tuning, it doesn’t freeze the pre-trained layers. This means that the weights of the entire BERT model (not just the final classification layer) are updated during training on the IMDb dataset."
      ],
      "metadata": {
        "id": "kzlgWTjjdAL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries and download the data\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertForQuestionAnswering, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "from transformers import pipeline\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, optimizer, scheduler, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Listing trainable parameters\n",
        "        trainable_params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "        print(f\"Number of trainable parameters: {len(trainable_params)}\")\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs} Loss: {loss.item()}')\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "    accuracy = correct_predictions.double() / len(test_loader.dataset)\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "def load_imdb_data(path):\n",
        "    data = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        dir_path = f\"{path}/{label}\"\n",
        "        for file in os.listdir(dir_path):\n",
        "            with open(f\"{dir_path}/{file}\", 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "                data.append((1 if label == 'pos' else 0, text))\n",
        "    return pd.DataFrame(data, columns=['label', 'text'])\n",
        "\n",
        "\n",
        "# Data Preparation - Tokenizing and padding\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=256):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.data.iloc[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "train_data = load_imdb_data('aclImdb/train')\n",
        "test_data = load_imdb_data('aclImdb/test')\n",
        "\n",
        "# Tokenizer and pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 8\n",
        "train_dataset = IMDBDataset(train_data, tokenizer)\n",
        "test_dataset = IMDBDataset(test_data, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Training setup\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_loader) * EPOCHS  # 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Training the model\n",
        "train_model(model, train_loader, optimizer, scheduler, EPOCHS)\n",
        "\n",
        "\n",
        "# Evaluating the model\n",
        "evaluate_model(model, test_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zS81lBcmpEh",
        "outputId": "0d68f74a-cc08-48db-ec22-0c923df8bbb2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 201\n",
            "Epoch 1/3 Loss: 0.3474839925765991\n",
            "Number of trainable parameters: 201\n",
            "Epoch 2/3 Loss: 0.7089992165565491\n",
            "Number of trainable parameters: 201\n",
            "Epoch 3/3 Loss: 0.835295557975769\n",
            "Accuracy: 0.9160\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: Gradual unfreezing\n",
        "\n",
        "\n",
        "Gradual unfreezing is a training strategy used in transfer learning. The idea is to fine-tune a pretrained model by initially freezing most of its layers, training the top (or newly added) layers, and progressively unfreezing more layers as training progresses. This allows the model to first learn the new task in a stable manner before updating the weights of the earlier, more general layers.\n",
        "\n",
        "When you load a pretrained model (like BERT), it has many layers, often with millions of parameters. These pretrained weights are optimized for a general task (e.g., language modeling in BERT). Initially, you freeze all layers except for the task-specific classifier (e.g., the final linear layer) by setting requires_grad=False for those layers. This ensures only the classifier is updated during the initial training phase.\n",
        "\n",
        "\n",
        "As training progresses, you progressively unfreeze earlier layers in the model, allowing their weights to be fine-tuned. This gradual unfreezing avoids destabilizing the model by ensuring that earlier, more general layers are only updated after the task-specific layers have been refined.\n",
        "\n",
        "* Each epoch (or after a certain number of epochs), you unfreeze one or more layers.\n",
        "* This can be done sequentially, layer by layer, or in groups (e.g., unfreezing an entire block of layers).\n"
      ],
      "metadata": {
        "id": "puPOIPr9WIjn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradual unfreezing for sentiment analysis and question answering with BERT on IMDb\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertForQuestionAnswering, AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "\n",
        "# Set up device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def load_imdb_data(path):\n",
        "    data = []\n",
        "    for label in ['pos', 'neg']:\n",
        "        dir_path = f\"{path}/{label}\"\n",
        "        for file in os.listdir(dir_path):\n",
        "            with open(f\"{dir_path}/{file}\", 'r', encoding='utf-8') as f:\n",
        "                text = f.read()\n",
        "                data.append((1 if label == 'pos' else 0, text))\n",
        "    return pd.DataFrame(data, columns=['label', 'text'])\n",
        "\n",
        "\n",
        "# Freeze all layers initially\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# keep the classifier layer trainable\n",
        "for name, param in model.named_parameters():\n",
        "    if \"classifier\" in name:\n",
        "        param.requires_grad = True\n",
        "\n",
        "\n",
        "# Unfreeze layers gradually\n",
        "def unfreeze_layers(model, epoch):\n",
        "    # Gradually unfreeze one encoder layer per epoch, starting from the last layer\n",
        "    if epoch < len(model.bert.encoder.layer):\n",
        "        for param in model.bert.encoder.layer[-(epoch + 1)].parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, optimizer, scheduler, epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        unfreeze_layers(model, epoch)  # Gradual unfreezing\n",
        "\n",
        "        # Debugging trainable parameters\n",
        "        trainable_params = list(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "        print(f\"Number of trainable parameters: {len(trainable_params)}\")\n",
        "\n",
        "        # Define optimizer with trainable parameters\n",
        "        optimizer = AdamW(trainable_params, lr=2e-5)\n",
        "\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{epochs} Loss: {loss.item()}')\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in test_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            _, preds = torch.max(outputs.logits, dim=1)\n",
        "            correct_predictions += torch.sum(preds == labels)\n",
        "\n",
        "    accuracy = correct_predictions.double() / len(test_loader.dataset)\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "\n",
        "# Data Preparation - Tokenizing and padding\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len=256):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        label, text = self.data.iloc[idx]\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "            truncation=True\n",
        "        )\n",
        "        return {\n",
        "            'text': text,\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "\n",
        "train_data = load_imdb_data('aclImdb/train')\n",
        "test_data = load_imdb_data('aclImdb/test')\n",
        "\n",
        "# Initialize the BERT tokenizer and model for sentiment analysis\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2).to(device)\n",
        "\n",
        "\n",
        "# Create DataLoaders\n",
        "BATCH_SIZE = 8\n",
        "train_dataset = IMDBDataset(train_data, tokenizer)\n",
        "test_dataset = IMDBDataset(test_data, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Training setup\n",
        "EPOCHS = 3\n",
        "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5, correct_bias=False)\n",
        "total_steps = len(train_loader) * EPOCHS  # 3 epochs\n",
        "scheduler = get_linear_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, optimizer, scheduler, EPOCHS)\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkxy5zTgBKNn",
        "outputId": "5bfae071-aed2-4de5-e2b8-cd931f3b22cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of trainable parameters: 201\n",
            "Epoch 1/3 Loss: 0.481783926486969\n",
            "Number of trainable parameters: 201\n",
            "Epoch 2/3 Loss: 0.014164907857775688\n",
            "Number of trainable parameters: 201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Question answering"
      ],
      "metadata": {
        "id": "fqX2NWDhZwPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "import os\n",
        "import tarfile\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertForQuestionAnswering\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load tokenizer and models\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "sentiment_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2).to(device)\n",
        "qa_model = BertForQuestionAnswering.from_pretrained(\"bert-large-uncased-whole-word-masking-finetuned-squad\").to(device)\n",
        "\n",
        "# Prepare data: extract 5 random reviews and labels\n",
        "def load_reviews(path, label):\n",
        "    reviews = []\n",
        "    for filename in os.listdir(path):\n",
        "        with open(os.path.join(path, filename), \"r\", encoding=\"utf-8\") as f:\n",
        "            reviews.append((f.read(), label))\n",
        "    return reviews\n",
        "\n",
        "# Function to ask questions about sentiment\n",
        "def ask_bert_review(text, question):\n",
        "    inputs = tokenizer.encode_plus(question, text, add_special_tokens=True, return_tensors=\"pt\").to(device)\n",
        "    answer_start_scores, answer_end_scores = qa_model(**inputs, return_dict=False)\n",
        "\n",
        "    answer_start = torch.argmax(answer_start_scores)\n",
        "    answer_end = torch.argmax(answer_end_scores) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
        "    return answer\n",
        "\n",
        "\n",
        "# Load positive and negative reviews\n",
        "positive_reviews = load_reviews(\"aclImdb/test/pos\", label=1)\n",
        "negative_reviews = load_reviews(\"aclImdb/test/neg\", label=0)\n",
        "\n",
        "\n",
        "# Combine and sample reviews\n",
        "all_reviews = positive_reviews + negative_reviews\n",
        "random.shuffle(all_reviews)\n",
        "sample_reviews = random.sample(all_reviews, 5)\n",
        "\n",
        "# Analyze reviews\n",
        "results = []\n",
        "\n",
        "for review, label in sample_reviews:\n",
        "    # Sentiment classification using pretrained bert - not fine tuned\n",
        "    inputs = tokenizer(review, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
        "    sentiment_output = sentiment_model(**inputs)\n",
        "    sentiment = torch.argmax(sentiment_output.logits, dim=1).item()\n",
        "    sentiment_label = \"positive\" if sentiment == 1 else \"negative\"\n",
        "    actual_label = \"positive\" if label == 1 else \"negative\"\n",
        "\n",
        "\n",
        "    # Store results\n",
        "    results.append({\n",
        "        \"review\": review,\n",
        "        \"predicted_sentiment\": sentiment_label,\n",
        "        \"actual_sentiment\": actual_label,\n",
        "        \"reason\": ask_bert_review(review, f\"Why is this review {sentiment_label}?\")\n",
        "    })\n",
        "\n",
        "# Print results\n",
        "for i, result in enumerate(results):\n",
        "    print(f\"Review {i + 1}:\")\n",
        "    print(f\"Text: {result['review'][:300]}...\")  # Truncate for display\n",
        "    print(f\"Predicted Sentiment: {result['predicted_sentiment']}\")\n",
        "    print(f\"Actual Sentiment: {result['actual_sentiment']}\")\n",
        "    print(f\"Reason: {result['reason']}\")\n",
        "    print(\"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udV3bG_TfkE6",
        "outputId": "e954ed27-7a73-4329-c777-c66f0105c1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1:\n",
            "Text: By 1971 it was becoming more and more obvious that Hammer film studios were on the way out . HANDS OF THE RIPPER is a case in point where even the idea smacks of desperation - The spirit of Jack The Ripper posses his own daughter ! Yeah okay no one was expecting a documentary but this plot seems to ...\n",
            "Predicted Sentiment: positive\n",
            "Actual Sentiment: negative\n",
            "Reason: \n",
            "\n",
            "\n",
            "Review 2:\n",
            "Text: I'm one of those gluttons for punishment when it comes to sitcoms these days-I still will check them out every once in a while.My observation is that most of them aren't very funny even the ones on major networks that are getting high ratings ,I just don't get who is finding them gut busting funny. ...\n",
            "Predicted Sentiment: positive\n",
            "Actual Sentiment: negative\n",
            "Reason: [CLS] why is this review positive ? [SEP]\n",
            "\n",
            "\n",
            "Review 3:\n",
            "Text: Ernesto is a man that makes a living out of duping other solid citizens of their hard earned money. Together with Manco, an older man with a lot of experience, he pulls out capers that allow him to make a decent living, but that is not making him a rich man by any means. Enter Federico, an older man...\n",
            "Predicted Sentiment: positive\n",
            "Actual Sentiment: positive\n",
            "Reason: [CLS] why is this review positive ? [SEP] ernesto is a man that makes a living out of duping other solid citizens of their hard earned money . together with manco , an older man with a lot of experience , he pulls out capers that allow him to make a decent living , but that is not making him a rich man by any means . enter federico , an older man who is more experience in the art of deception . together with the younger ernesto they prove a winning combination . that only lasts until pilar , federico ' s former love interest , appears in the picture . < br / > < br / > this spanish film directed by miguel bardem , is light in tone and pleasant to sit through\n",
            "\n",
            "\n",
            "Review 4:\n",
            "Text: This is a wonderful film taking place during the romantic period of the Civil War. This film is a must see for Eastwood Fans and Eastwood claims this is one of his most favorite films that he did. I couldn't agree more. Watch out! This is a spoiler- Eastwood does die in the end. Eastwood and directo...\n",
            "Predicted Sentiment: positive\n",
            "Actual Sentiment: positive\n",
            "Reason: \n",
            "\n",
            "\n",
            "Review 5:\n",
            "Text: This film is the best kung fu film of all time. Although there is not wire-work and special effects like those used in Crouching Tiger, this movie uses ingenuity and creative camera-work to create memorable fighting moments, and the fight scenes are well choreographed and tight. There is a ton of ac...\n",
            "Predicted Sentiment: positive\n",
            "Actual Sentiment: positive\n",
            "Reason: this film is the best kung fu film of all time\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TO-DO\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Parts 1 and 2\n",
        "\n",
        "* **Experiment with Different Pretrained Models**:\n",
        "   - Try other transformer-based models like `distilbert-base-uncased` or `roberta-base`. Compare their performance with `bert-base-uncased`.\n",
        "\n",
        "* **Adjust Hyperparameters**:\n",
        "   - Experiment with learning rates (e.g., `1e-5`, `5e-5`, `2e-6`) and batch sizes to observe their impact on model performance.\n",
        "   - Evaluate how changing the number of epochs affects validation accuracy and loss.\n",
        "\n",
        "\n",
        "* **Comparative Analysis**:\n",
        "   - Compare the performance of fine-tuning with gradual unfreezing to fine-tuning with all layers unfrozen from the start.\n",
        "   - Discuss the computational trade-offs (time vs. accuracy) and performance differences.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Part 3\n",
        "\n",
        "* **Manual Review**:\n",
        "   - Manually analyze a few QA outputs to assess whether the model’s reasoning aligns with human judgment. Highlight discrepancies and discuss their causes.\n",
        "\n",
        "* **Fine-tuned BERT**:\n",
        "   - Replace the simple pretrained BERT with your best fine-tuned model and/or replace BERT with other transformer-based model for Q&A\n",
        "\n",
        "* **Prompt Engineering**:\n",
        "   - Experiment with different phrasing for the question (e.g., \"What makes this review positive/negative?\" or \"Explain the sentiment of this review.\"). Discuss how the phrasing affects the model's answers.\n",
        "\n",
        "* **Entity-Specific Analysis**:\n",
        "   - Modify the question to focus on specific entities or aspects in the review (e.g., \"What does the review say about the acting?\" or \"Why is the plot criticized?\"). Discuss the model’s ability to handle nuanced questions.\n"
      ],
      "metadata": {
        "id": "dQR49-ZYmrXQ"
      }
    }
  ]
}